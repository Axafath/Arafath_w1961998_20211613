# -*- coding: utf-8 -*-
"""W1961998_20211613_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WAVW1B-oMmbkK7uNM8lrLdhDJ3eIktsy

# **Clustering and Predicting IIT Student Stress Levels Using Machine Learning**

# **Importing the Libraries**
"""

import pandas as pd
import numpy as np
import math
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
from scipy.stats import skew
from yellowbrick.cluster import KElbowVisualizer
from sklearn.feature_selection import VarianceThreshold
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.linear_model import LogisticRegression
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, silhouette_score, davies_bouldin_score, calinski_harabasz_score

"""# **Loading the Dataset through Google Drive**"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Import and store in the df variable
df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/FYP/IIT_Student Stress_Data.csv')

"""# **Data Preprocessing**"""

# Show first 5 rows of data
df.head()

# Add a 'Number' column starting from 1
df.insert(0, 'Number', range(1, len(df) + 1))

df.head()

"""### **Removal of non-contributory columns**"""

# Drop the first column (timestamp)
df = df.drop(df.columns[1], axis=1)

"""### **Renaming and structuring**"""

# Renaming the demographic columns
df.columns.values[1] = "Age"
df.columns.values[2] = "Gender"
df.columns.values[3] = "Field_of_Study"
df.columns.values[4] = "Employment_Status"
df.columns.values[5] = "Academic_Year"

# Renaming the stress columns to Q1, Q2, Q3, ...
for i in range(6, len(df.columns)):
    df.columns.values[i] = f"Q{i - 5}"

print(df.head())

# Show first 5 rows of data after renaming
df.head()

"""### **Handling missing values**"""

#to check the sum of missing values
df.isna().sum()

# Define columns containing the survey responses (Q1-Q27), excluding demographics
question_cols = [f'Q{i}' for i in range(1, 28)]

# Fill missing values with rounded column mean
df[question_cols] = df[question_cols].fillna(df[question_cols].mean().round())
df[question_cols] = df[question_cols].astype(int)

# Ensure all values stay within 1 to 4
df[question_cols] = df[question_cols].clip(lower=1, upper=4)

#to check the sum of missing values after handling
df.isna().sum()

#to check the sum of missing values after handling it
df.isna().sum()

"""### **Handling uniform(inactive) responses**"""

# Remove rows with uniform responses
initial_rows = df.shape[0]
df = df[df[question_cols].nunique(axis=1) > 1]
removed_rows = initial_rows - df.shape[0]
print(f"{removed_rows} rows removed due to uniform responses.")

"""### **Skewness & transformation**"""

# Seperating the numerical varaible
numeric_cols = df.select_dtypes(include=['number']).columns

# Calculating and printing skewness for each numeric variable
for feature in numeric_cols:
    feature_skewness = skew(df[feature])
    print(f"Skewness of {feature}: {feature_skewness:.2f}")

"""No variables have been identified with a high skew, therefore skew transformation is not necessary

### **Standard Scaler**
"""

#Scale numerical features
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df[numeric_cols])

"""### **Correlation Matrix**"""

# Calculate the correlation matrix
corr_matrix = df[question_cols].corr()

correlation_threshold_pairwise = 0.8

# list of features that would be removed
features_removed = set()

# Start from the upper triangle to avoid duplicate pairs and self-correlation
for i in range(len(corr_matrix.columns)):
    for j in range(i):
        if abs(corr_matrix.iloc[i, j]) > correlation_threshold_pairwise:
            colname_i = corr_matrix.columns[i]
            colname_j = corr_matrix.columns[j]

            features_removed.add(colname_i)


# list of features that would remain
all_features = set(question_cols)
features_kept = list(all_features - features_removed)

# Print the results
print(f"Features removed (threshold > {correlation_threshold_pairwise}): {list(features_removed)}")
print(f"\nFeatures remained: {features_kept}")

"""### **Feature Variance Analysis**"""

# Calculate variance for each question
variances = df[question_cols].var()

threshold = 0.5

# Plot
plt.figure(figsize=(16, 6))
variances.plot(kind='bar', color='steelblue')
plt.axhline(y=threshold, color='gray', linestyle='--', linewidth=1.2, label=f'Threshold = {threshold}')
plt.title("Variance of Each Question (Before Feature Selection)")
plt.ylabel("Variance")
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.legend()
plt.tight_layout()
plt.show()

"""Note: No features were removed during feature scaling, indicating that all features had a variance over the specified threshold (0.5).

## **Exploratory Data Analysis**
"""

# Basic statistics for numerical columns
print(df.describe())

#Feature Engineering: Average of each section for EDA (Academic, Financial, Social...)

# Calculate average stress levels for each section
df['Academic_Stress_Avg'] = df[['Q1', 'Q2', 'Q3', 'Q4']].mean(axis=1)
df['Social_Stress_Avg'] = df[['Q5', 'Q6', 'Q7', 'Q8']].mean(axis=1)
df['Financial_Stress_Avg'] = df[['Q9', 'Q10', 'Q11', 'Q12']].mean(axis=1)
df['Personal_Stress_Avg'] = df[['Q13', 'Q14', 'Q15', 'Q16', 'Q17']].mean(axis=1)
df['Environmental_Stress_Avg'] = df[['Q18', 'Q19', 'Q20', 'Q21', 'Q22']].mean(axis=1)
df['Coping_Strategies_Avg'] = df[['Q23', 'Q24', 'Q25', 'Q26', 'Q27']].mean(axis=1)

"""### **Average Stress Level By Category**"""

# Create a dictionary of stress categories and their mean values
avg_stress = {
    'Academic': df['Academic_Stress_Avg'].mean(),
    'Social': df['Social_Stress_Avg'].mean(),
    'Financial': df['Financial_Stress_Avg'].mean(),
    'Personal/Emotional': df['Personal_Stress_Avg'].mean(),
    'Environmental': df['Environmental_Stress_Avg'].mean(),
    'Coping Strategies': df['Coping_Strategies_Avg'].mean()
}

# Bar plot
plt.figure(figsize=(10, 5))
plt.bar(avg_stress.keys(), avg_stress.values(), color='skyblue')
plt.title('Average Stress Level by Category')
plt.ylabel('Mean Score (1 = Low, 4 = High)')
plt.ylim(1, 4)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()  #
plt.show()

"""### **Distirbution of responses by Demographics**"""

for col in ['Gender', 'Academic_Year', 'Field_of_Study', 'Age', 'Employment_Status']:
    sns.countplot(x=col, data=df)
    plt.title(f"Distribution of {col}")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

"""### **Distribution of each question**"""

# Plot all question distributions together
n_cols_plot = 4
n_rows_plot = math.ceil(len(question_cols) / n_cols_plot)
fig, axes = plt.subplots(n_rows_plot, n_cols_plot, figsize=(n_cols_plot * 4, n_rows_plot * 3))
axes = axes.flatten()

for i, col in enumerate(question_cols):
    sns.countplot(x=col, data=df, ax=axes[i], hue=col, palette='viridis', legend=False)
    axes[i].set_title(f'Distribution of {col}')
    axes[i].set_xlabel('Response')
    axes[i].set_ylabel('Count')

# Hide any unused subplots
for j in range(i + 1, len(axes)):
    axes[j].axis('off')

plt.tight_layout()
plt.show()

"""### **Distribution of Stress Levels by Each Section (Academic, Financial, etc..)**"""

stress_columns = ["Academic_Stress_Avg", "Social_Stress_Avg", "Financial_Stress_Avg", "Personal_Stress_Avg",
                 "Environmental_Stress_Avg", "Coping_Strategies_Avg"]

for column in stress_columns:
    plt.figure(figsize=(8, 6))
    sns.histplot(df[column], bins=10, kde=True)
    plt.title(f"Distribution of {column.replace('_Avg', '')}")
    plt.xlabel("Stress Level (1 = Low, 4 = High)")
    plt.ylabel("Number of Students")
    plt.show()

"""### **Distribution of Stress Levels By Gender**"""

for column in stress_columns:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x="Gender", y=column, data=df)
    plt.title(f"{column.replace('_Avg', '')} by Gender")
    plt.xlabel("Gender")
    plt.ylabel("Stress Level (1 = Low, 4 = High)")
    plt.show()

"""### **Distribution of Stress Levels By Academic Year**"""

for column in stress_columns:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x="Academic_Year", y=column, data=df)
    plt.title(f"{column.replace('_Avg', '')} by Academic Year")
    plt.xlabel("Academic Year")
    plt.ylabel("Stress Level (1 = Low, 4 = High)")
    plt.show()

"""### **Distribution of Stress Levels By Degree Pathway**"""

for column in stress_columns:
    plt.figure(figsize=(12, 6))
    sns.boxplot(x="Field_of_Study", y=column, data=df)
    plt.title(f"{column.replace('_Avg', '')} by Degree Pathway")
    plt.xlabel("Degree Pathway")
    plt.ylabel("Stress Level (1 = Low, 4 = High)")
    plt.xticks(rotation=45, ha='right')
    plt.show()

"""### **Correlation between Stress Sections**"""

# Correlation matrix for section-wise stress averages
stress_columns = ["Academic_Stress_Avg", "Social_Stress_Avg", "Financial_Stress_Avg", "Personal_Stress_Avg",
                 "Environmental_Stress_Avg", "Coping_Strategies_Avg"]

corr_matrix = df[stress_columns].corr()

# Heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Between Stress Sections")
plt.show()

"""### **Correlation heatmap between individual features**


"""

# Correlation heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(df[question_cols].corr(), cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()

"""# **K Means Clustering - Unsupervised ML**

## **Elbow method to determine the number of clusters**
"""

#Elbow Method
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(scaled_data)
    wcss.append(kmeans.inertia_)

# Elbow graph
plt.plot(range(1, 11), wcss, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('WCSS')

"""## **Silhouette Analysis to determine the number of clusters**"""

#Silhouette Analysis
silhouette_scores = []
for i in range(2, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    cluster_labels = kmeans.fit_predict(scaled_data)
    silhouette_scores.append(silhouette_score(scaled_data, cluster_labels))

# Plotting the Silhouette scores
plt.plot(range(2, 11), silhouette_scores, marker='o')
plt.title('Silhouette Analysis for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Score')
plt.show()

"""## **Comparing between 2 or 3 Clusters**"""

# list to collect results
results = []

# Performing K-means for 2, 3 clusters
for k in range(2, 4):
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(scaled_data)

    # Scores
    silhouette = silhouette_score(scaled_data, labels)
    dbi = davies_bouldin_score(scaled_data, labels)
    chi = calinski_harabasz_score(scaled_data, labels)

    # Append results
    results.append({
        "k": k,
        "Silhouette Score": round(silhouette, 4),
        "Davies-Bouldin Index": round(dbi, 4),
        "Calinski-Harabasz Index": round(chi, 2)
    })

# Convert results to DataFrame and display
results_df = pd.DataFrame(results)
print(results_df)

"""•	Silhouette: Peak value.
	•	DBI: Lowest value.
	•	CHI: Highest value.

**PCA Visualization of 2 Clusters vs 3 Clusters**
"""

# Get Cluster Labels for k=2
kmeans_k2 = KMeans(n_clusters=2, random_state=42, n_init=10)
cluster_labels_k2 = kmeans_k2.fit_predict(scaled_data)

# Get the labels for k=3
kmeans_k3 = KMeans(n_clusters=3, random_state=42, n_init=10)
cluster_labels_k3 = kmeans_k3.fit_predict(scaled_data)

# Create DataFrames with Cluster Labels
df_k2 = pd.DataFrame({'Cluster': cluster_labels_k2}, index=df.index)
df_k3 = pd.DataFrame({'Cluster': cluster_labels_k3}, index=df.index)

# PCA for visualization
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(scaled_data)
df_k2[['PCA1', 'PCA2']] = X_pca
df_k3[['PCA1', 'PCA2']] = X_pca

# Side-by-side PCA plots
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

sns.scatterplot(x='PCA1', y='PCA2', hue='Cluster', data=df_k2,
                palette='Set1', ax=axes[0], s=60, alpha=0.8)
axes[0].set_title("PCA - k=2 Clusters")
axes[0].set_xlabel("PCA 1")
axes[0].set_ylabel("PCA 2")
axes[0].grid(True, linestyle='--', alpha=0.5)

sns.scatterplot(x='PCA1', y='PCA2', hue='Cluster', data=df_k3,
                palette='Set2', ax=axes[1], s=60, alpha=0.8)
axes[1].set_title("PCA - k=3 Clusters")
axes[1].set_xlabel("PCA 1")
axes[1].set_ylabel("PCA 2")
axes[1].grid(True, linestyle='--', alpha=0.5)

plt.tight_layout()
plt.show()

"""Althought K = 2 evaluation metrics outperforms that of the k = 3 evaluation metric, k = 3 was chosen due to its diversification which suits the project scope better

### **Performing K Means Clustering**
"""

# Final clustering with k=3
optimal_k = 3
kmeans = KMeans(n_clusters=optimal_k, random_state=0)
df['Cluster'] = kmeans.fit_predict(scaled_data)

# Cluster means
print("\nMean values for each cluster:")
print(df.groupby('Cluster')[question_cols].mean())

# Demographic distribution
for col in ['Gender', 'Academic_Year', 'Field_of_Study']:
    print(f"\n{col} by Cluster:")
    print(pd.crosstab(df['Cluster'], df[col]))

"""### **PCA Visualization for 3 Clusters**"""

# PCA for visualization
pca = PCA(n_components=2)
pca_data = pca.fit_transform(scaled_data)


# Final scatterplot
plt.figure(figsize=(10, 7))
sns.scatterplot(data=df, x=X_pca[:, 0], y=X_pca[:, 1], hue='Cluster', palette='Set2', s=70, alpha=0.85)
plt.title("Final Clustering (k=3) - PCA Visualization")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

print(f"--- Evaluation Metrics for K-Means k=3 ---")

# Silhouette Score
silhouette_avg_k3 = silhouette_score(scaled_data, cluster_labels_k3)
print(f"\nSilhouette Score: {silhouette_avg_k3:.4f}")

# Davies-Bouldin Index
dbi_score_k3 = davies_bouldin_score(scaled_data, cluster_labels_k3)
print(f"Davies-Bouldin Index: {dbi_score_k3:.4f}")

# Calinski-Harabasz Index
chi_score_k3 = calinski_harabasz_score(scaled_data, cluster_labels_k3)
print(f"Calinski-Harabasz Index: {chi_score_k3:.2f}")

"""### **Radar chart**"""

import plotly.graph_objects as go
import plotly.express as px

categories = ['Academic', 'Social', 'Financial', 'Personal', 'Environmental', 'Coping']
stress_avg_cols = [
    'Academic_Stress_Avg',
    'Social_Stress_Avg',
    'Financial_Stress_Avg',
    'Personal_Stress_Avg',
    'Environmental_Stress_Avg',
    'Coping_Strategies_Avg'
]

# Define a color palette for the clusters
colors = px.colors.qualitative.Set2

for i, cluster in enumerate(sorted(df['Cluster'].unique())):
    fig = go.Figure() # Create a new figure for each cluster

    cluster_data = df[df['Cluster'] == cluster][stress_avg_cols].mean().tolist()

    # Close the loop for radar chart
    cluster_data += cluster_data[:1]

    fig.add_trace(go.Scatterpolar(
        r=cluster_data,
        theta=categories + [categories[0]],
        fill='toself',
        name=f'Cluster {cluster}',
        marker=dict(color=colors[i % len(colors)]) # Assign a color from the palette
    ))

    fig.update_layout(
        polar=dict(
            radialaxis=dict(visible=True, range=[1,4])
        ),
        showlegend=True,
        title=f"Stress Profile for Cluster {cluster}" # Update title for each cluster
    )

    fig.show()

"""# **Hierarchical Clustering - Unsupervised ML**

## **Performing hierarchical clustering**
"""

linked = linkage(scaled_data, method='ward') # minimises variance within clusters to ensure they have similar properties

# Plot the dendrogram
plt.figure(figsize=(15, 8))
dendrogram(linked,
           orientation='top',
           distance_sort='decreasing',
           show_leaf_counts=True)
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
plt.show()

# Choose 3 clusters to evaluate
from scipy.cluster.hierarchy import fcluster
num_clusters_hac = 3

# Get the cluster labels by cutting the dendrogram
hac_labels = fcluster(linked, num_clusters_hac, criterion='maxclust')

"""### **Evaluating the model**"""

print(f"--- Evaluation Metrics for Hierarchy k=3 ---")

# Silhouette Score
silhouette_avg = silhouette_score(scaled_data, hac_labels)
print(f"\nSilhouette Score: {silhouette_avg:.4f}")

# Davies-Bouldin Index
dbi_score = davies_bouldin_score(scaled_data, hac_labels)
print(f"Davies-Bouldin Index: {dbi_score:.4f}")

# Calinski-Harabasz Index
chi_score = calinski_harabasz_score(scaled_data, hac_labels)
print(f"Calinski-Harabasz Index: {chi_score:.2f}")

"""K Means Clustering's evaluation metrics of

Silhouette Score: 0.0993
Davies-Bouldin Index: 2.4131
Calinski-Harabasz Index: 21.86

outperforms that of the hierarchy clustering model's evaluation metric

### **Top Contributing Questions for Each Cluster**
"""

# Group by cluster and calculate mean of question columns
cluster_means = df.groupby('Cluster')[question_cols].mean()

# Plot for Cluster 0
plt.figure(figsize=(12, 5))
cluster_means.loc[0].sort_values(ascending=False).plot(kind='bar', color='steelblue')
plt.title('Top Contributing Questions for Cluster 0')
plt.ylabel('Mean Score')
plt.xlabel('Question')
plt.xticks(rotation=45)
plt.ylim(1, 4)
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

# Plot for Cluster 1
plt.figure(figsize=(12, 5))
cluster_means.loc[1].sort_values(ascending=False).plot(kind='bar', color='darkorange')
plt.title('Top Contributing Questions for Cluster 1')
plt.ylabel('Mean Score')
plt.xlabel('Question')
plt.xticks(rotation=45)
plt.ylim(1, 4)
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

# Plot for Cluster 2
plt.figure(figsize=(12, 5))
cluster_means.loc[2].sort_values(ascending=False).plot(kind='bar', color='forestgreen')
plt.title('Top Contributing Questions for Cluster 2')
plt.ylabel('Mean Score')
plt.xlabel('Question')
plt.xticks(rotation=45)
plt.ylim(1, 4)
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

"""# **Decision Tree Classifier - Supervised ML**

### **Defining the target variables and train-test split**
"""

# Define features and target
X = df[question_cols]
y = df['Cluster']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

"""### **Developing the model**"""

dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)

"""### **Evaluating the model**"""

# Make predictions on the test data
y_pred = dt.predict(X_test)

print("--- Default Decision Tree Performance ---")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""### **Confusion Matrix Plot**"""

conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted Cluster")
plt.ylabel("Actual Cluster")
plt.show()

# Visualize the Tree
plt.figure(figsize=(20,10))
plot_tree(dt, feature_names=question_cols, class_names=[str(i) for i in sorted(df['Cluster'].unique())], filled=True, rounded=True)
plt.title("Decision Tree Visualization")
plt.show()

"""### **Feature importance for Decision Tree model**"""

# Get feature importances from the Decision Tree model
dt_feature_importances = dt.feature_importances_

feature_names = X_train.columns
dt_importance_series = pd.Series(dt_feature_importances, index=feature_names)

# Plot
plt.figure(figsize=(12, 6))
dt_importance_series.sort_values(ascending=False).plot(kind='bar')
plt.title("Decision Tree Feature Importance")
plt.xlabel("Features")
plt.ylabel("Importance Score")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""# **Logistic Regression - Supervised ML**"""

# Initialize and train the Logistic Regression model
logreg_model = LogisticRegression(max_iter=200, random_state=42)

# Train the model on the training data
logreg_model.fit(X_train, y_train)

"""### **Evaluating the model**"""

# Make predictions on the test data
y_pred_logreg = logreg_model.predict(X_test)

print("--- Logistic Regression Performance ---")
print("Accuracy:", accuracy_score(y_test, y_pred_logreg))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_logreg))

"""### **Confusion Matrix Plot**"""

conf_matrix_logreg = confusion_matrix(y_test, y_pred_logreg)
sns.heatmap(conf_matrix_logreg, annot=True, fmt='d', cmap='Blues')
plt.title("Logistic Regression Confusion Matrix")
plt.xlabel("Predicted Cluster")
plt.ylabel("Actual Cluster")
plt.show()

"""Logistic Regression's evaluation metrics of

Accuracy: 0.9230769230769231

outperforms that of the Decision Tree Classifier's evaluation metric

### **Feature importance for Logistic Regression model**
"""

logreg_coefficients = logreg_model.coef_

logreg_importance_scores = np.mean(np.abs(logreg_coefficients), axis=0)


feature_names = X_train.columns
logreg_importance_series = pd.Series(logreg_importance_scores, index=feature_names)


# Plot
plt.figure(figsize=(12, 6))
logreg_importance_series.sort_values(ascending=False).plot(kind='bar', color='salmon')
plt.title("Logistic Regression Feature Importance")
plt.xlabel("Features")
plt.ylabel("Average Absolute Coefficient")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# First, make predictions on the entire dataset (not just X_test)
X_all = df[question_cols]  # Use question_cols instead of feature_cols
predicted_labels = logreg_model.predict(X_all)

# Add the predicted labels as a new column
df['Predicted_Cluster'] = predicted_labels

# If you haven't added the actual cluster labels yet
# Make sure 'Cluster' column exists based on your earlier KMeans results
# e.g., df['Cluster'] = kmeans.labels_

df.head()

"""# **Exporting the csv to Powerbi**"""

df.to_csv('Dataset.csv', index=False)

from google.colab import files
files.download('Dataset.csv')